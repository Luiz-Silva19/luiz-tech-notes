---
id: distributed-systems-cap
title: Teorema CAP
sidebar_label: Teorema CAP
---

## O que Ã© o Teorema CAP?

O **Teorema CAP** (tambÃ©m conhecido como Teorema de Brewer) afirma que Ã© **impossÃ­vel** para um sistema distribuÃ­do garantir simultaneamente as trÃªs propriedades:

- **C**onsistency (ConsistÃªncia)
- **A**vailability (Disponibilidade)
- **P**artition Tolerance (TolerÃ¢ncia a PartiÃ§Ãµes)

Em caso de **partiÃ§Ã£o de rede**, vocÃª deve escolher entre **ConsistÃªncia** ou **Disponibilidade**.

## As TrÃªs Propriedades

### ğŸ”„ Consistency (ConsistÃªncia)

**DefiniÃ§Ã£o**: Todos os nÃ³s veem os mesmos dados ao mesmo tempo.

- ApÃ³s uma escrita bem-sucedida, todas as leituras subsequentes retornam o valor atualizado
- NÃ£o hÃ¡ dados "desatualizados" no sistema
- Equivalente a ter uma Ãºnica cÃ³pia atualizada dos dados

**Exemplo**:

```
Cliente 1 escreve: X = 10
Cliente 2 lÃª X = 10  âœ… (nÃ£o lÃª valor antigo)
Cliente 3 lÃª X = 10  âœ… (nÃ£o lÃª valor antigo)
```

### âœ… Availability (Disponibilidade)

**DefiniÃ§Ã£o**: Todo request recebe uma resposta (sucesso ou falha), sem garantia de que contÃ©m a versÃ£o mais recente dos dados.

- O sistema sempre responde Ã s requisiÃ§Ãµes
- NÃ£o hÃ¡ timeouts ou erros por indisponibilidade
- Mesmo que alguns nÃ³s estejam offline

**Exemplo**:

```
3 nÃ³s no cluster
1 nÃ³ cai
Sistema continua respondendo com os 2 nÃ³s restantes âœ…
```

### ğŸ”Œ Partition Tolerance (TolerÃ¢ncia a PartiÃ§Ãµes)

**DefiniÃ§Ã£o**: O sistema continua operando mesmo quando hÃ¡ falha de comunicaÃ§Ã£o entre nÃ³s.

- PartiÃ§Ã£o de rede: alguns nÃ³s nÃ£o conseguem se comunicar com outros
- O sistema deve continuar funcionando mesmo com a partiÃ§Ã£o
- Em sistemas distribuÃ­dos reais, partiÃ§Ãµes **vÃ£o acontecer**

**Exemplo**:

```
Datacenter A  <--X-->  Datacenter B
(rede particionada)

Sistema continua operando em ambos os datacenters
```

## Por que sÃ³ Ã© possÃ­vel escolher 2?

### CenÃ¡rio: PartiÃ§Ã£o de Rede

Imagine um sistema com 2 nÃ³s: **A** e **B**

```
Cliente 1 ---> [NÃ³ A]  <--X-->  [NÃ³ B] <--- Cliente 2
                (nÃ£o conseguem se comunicar)
```

**Cliente 1** escreve `X = 10` no NÃ³ A

Agora temos duas opÃ§Ãµes:

#### OpÃ§Ã£o 1: CP (Consistency + Partition Tolerance)

- NÃ³ B **nÃ£o responde** ao Cliente 2 (sacrifica Disponibilidade)
- Espera reconectar com NÃ³ A para garantir dados consistentes
- **Resultado**: Sistema **indisponÃ­vel** mas **consistente**

#### OpÃ§Ã£o 2: AP (Availability + Partition Tolerance)

- NÃ³ B **responde** ao Cliente 2 com valor antigo de X
- **Resultado**: Sistema **disponÃ­vel** mas **inconsistente**

#### OpÃ§Ã£o 3: CA (Consistency + Availability)

- **ImpossÃ­vel** na presenÃ§a de partiÃ§Ã£o de rede
- SÃ³ funciona em sistemas nÃ£o-distribuÃ­dos (single node)

## CombinaÃ§Ãµes CAP

### CP - Consistency + Partition Tolerance

**Sacrifica**: Disponibilidade

**Comportamento**:

- Sistema pode rejeitar requests durante partiÃ§Ã£o
- Garante que dados sÃ£o sempre consistentes
- Pode retornar erros ou timeouts

**Exemplos**:

- **MongoDB**: Em modo de replica set padrÃ£o
- **HBase**: Prioriza consistÃªncia
- **Redis** (com replicaÃ§Ã£o sÃ­ncrona)
- **Zookeeper**: Consensus-based

**Quando usar**:

- OperaÃ§Ãµes financeiras (nÃ£o pode haver inconsistÃªncia)
- Sistemas que exigem dados corretos acima de tudo
- Quando Ã© aceitÃ¡vel que o sistema fique indisponÃ­vel temporariamente

### AP - Availability + Partition Tolerance

**Sacrifica**: ConsistÃªncia (forte)

**Comportamento**:

- Sistema sempre responde, mesmo durante partiÃ§Ã£o
- Pode retornar dados desatualizados
- Usa eventual consistency

**Exemplos**:

- **Cassandra**: Disponibilidade prioritÃ¡ria
- **DynamoDB**: Eventual consistency por padrÃ£o
- **CouchDB**: Multi-master replication
- **Riak**: Disponibilidade alta

**Quando usar**:

- Redes sociais (ok mostrar dados ligeiramente desatualizados)
- Sistemas de cache
- DNS
- Quando disponibilidade Ã© crÃ­tica

### CA - Consistency + Availability

**Sacrifica**: Partition Tolerance

**Comportamento**:

- Funciona apenas sem partiÃ§Ãµes de rede
- Na prÃ¡tica, sÃ³ funciona em sistemas nÃ£o-distribuÃ­dos

**Exemplos**:

- **Bancos de dados tradicionais** (PostgreSQL, MySQL) em single node
- Sistemas monolÃ­ticos

**Realidade**:

- Em sistemas distribuÃ­dos reais, partiÃ§Ãµes **vÃ£o acontecer**
- Logo, **CA nÃ£o Ã© uma opÃ§Ã£o viÃ¡vel** para sistemas distribuÃ­dos verdadeiros

## PACELC - ExtensÃ£o do CAP

O teorema **PACELC** estende o CAP considerando latÃªncia:

**P**artition: Em caso de partiÃ§Ã£o, escolha entre **A** (Availability) ou **C** (Consistency)

**E**lse: Caso contrÃ¡rio (sem partiÃ§Ã£o), escolha entre **L** (Latency) ou **C** (Consistency)

### Exemplos PACELC:

**PA/EL** (Cassandra):

- Durante partiÃ§Ã£o: Availability
- Sem partiÃ§Ã£o: Lower Latency
- Trade-off: Eventual consistency

**PC/EC** (HBase, BigTable):

- Durante partiÃ§Ã£o: Consistency
- Sem partiÃ§Ã£o: Consistency (maior latÃªncia)
- Trade-off: Maior latÃªncia para garantir consistÃªncia

## Configurabilidade

Muitos sistemas modernos permitem **configurar** o trade-off:

### Cassandra

```cql
-- Write com consistÃªncia forte
INSERT INTO users VALUES (...)
USING CONSISTENCY QUORUM;

-- Read com eventual consistency
SELECT * FROM users
USING CONSISTENCY ONE;
```

### DynamoDB

- **Strong Consistent Reads**: Garante leitura mais recente
- **Eventually Consistent Reads**: Menor latÃªncia, pode ler valor antigo

## Conceitos Importantes

### ğŸ”„ Eventual Consistency

- ApÃ³s cessarem as escritas, eventualmente todos os nÃ³s convergem
- NÃ£o hÃ¡ garantia de **quando** isso acontecerÃ¡
- Usado em sistemas AP

### ğŸ”’ Strong Consistency

- Toda leitura retorna a escrita mais recente
- Requer coordenaÃ§Ã£o entre nÃ³s
- Usado em sistemas CP

### âš–ï¸ Quorum

- Maioria dos nÃ³s deve confirmar operaÃ§Ã£o
- Balanceia consistÃªncia e disponibilidade
- `W + R > N` garante consistÃªncia
  - W: write replicas
  - R: read replicas
  - N: total replicas

## ConclusÃ£o

O Teorema CAP nÃ£o Ã© sobre escolher 2 de 3 propriedades para sempre, mas sim entender os **trade-offs** que vocÃª faz **durante uma partiÃ§Ã£o de rede**.

Na prÃ¡tica:

- âœ… **Sempre** tenha Partition Tolerance (P) em sistemas distribuÃ­dos
- âš–ï¸ **Escolha** entre Consistency (C) e Availability (A) durante partiÃ§Ãµes
- ğŸ”§ Muitos sistemas modernos permitem **configurar** esse trade-off por operaÃ§Ã£o

## ğŸ“š ReferÃªncias e Recursos

### Paper Original

- **<a href="https://users.ece.cmu.edu/~adrian/731-sp04/readings/GL-cap.pdf" target="_blank" rel="noopener noreferrer">Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services</a>** - Gilbert & Lynch (2002)
- **<a href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/" target="_blank" rel="noopener noreferrer">CAP Twelve Years Later: How the "Rules" Have Changed</a>** - Eric Brewer (2012)

### Artigos Explicativos

- **<a href="https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html" target="_blank" rel="noopener noreferrer">Please stop calling databases CP or AP</a>** - Martin Kleppmann
- **<a href="https://codahale.com/you-cant-sacrifice-partition-tolerance/" target="_blank" rel="noopener noreferrer">You Can't Sacrifice Partition Tolerance</a>** - Coda Hale
- **<a href="http://robertgreiner.com/cap-theorem-revisited/" target="_blank" rel="noopener noreferrer">CAP Theorem Revisited</a>** - Robert Greiner

### DocumentaÃ§Ã£o de Sistemas

- **<a href="https://cassandra.apache.org/doc/latest/cassandra/architecture/overview.html" target="_blank" rel="noopener noreferrer">Cassandra Architecture</a>** - AP system
- **<a href="https://www.mongodb.com/docs/manual/core/read-preference/" target="_blank" rel="noopener noreferrer">MongoDB Consistency</a>** - CP system
- **<a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html" target="_blank" rel="noopener noreferrer">DynamoDB Consistency</a>** - Configurable

---

**PrÃ³ximo**: [Modelos de ConsistÃªncia](consistency-models.md)
