---
id: distributed-systems-consensus
title: Consenso
sidebar_label: Consenso
---

## O que Ã© Consenso em Sistemas DistribuÃ­dos?

**Consenso** Ã© o processo de fazer mÃºltiplos nÃ³s em um sistema distribuÃ­do concordarem sobre um Ãºnico valor ou estado, mesmo na presenÃ§a de falhas.

## Por que Consenso Ã© Importante?

### Problemas que Consenso Resolve

#### 1. ReplicaÃ§Ã£o de Estado

```
Node A: balance = $100
Node B: balance = $100  â† Todos concordam sobre saldo
Node C: balance = $100
```

#### 2. Ordem de OperaÃ§Ãµes

```
Cliente 1: SET x = 1
Cliente 2: SET x = 2

Todos os nÃ³s devem concordar: qual operaÃ§Ã£o vem primeiro?
```

#### 3. Membership Changes

```
Cluster: [A, B, C]

Adicionar D
â†’ Todos devem concordar que cluster agora Ã© [A, B, C, D]
```

#### 4. Leader Election

```
MÃºltiplos nÃ³s candidatos a lÃ­der
â†’ Consenso sobre quem Ã© o lÃ­der
```

## Propriedades do Consenso

### 1. Agreement (Acordo)

Todos os nÃ³s corretos decidem o **mesmo valor**.

```
Node A decide: X
Node B decide: X  â† Mesmo valor
Node C decide: X
```

### 2. Validity (Validade)

Se todos os nÃ³s propÃµem o mesmo valor V, entÃ£o a decisÃ£o Ã© V.

```
Todos propÃµem: X
DecisÃ£o: X âœ…

Todos propÃµem: X
DecisÃ£o: Y âŒ (violaÃ§Ã£o)
```

### 3. Termination (TÃ©rmino)

Todo nÃ³ correto eventualmente decide algum valor.

```
Algoritmo nÃ£o pode ficar bloqueado forever
```

### 4. Integrity (Integridade)

Cada nÃ³ decide no mÃ¡ximo uma vez.

```
Node A: decide X
Node A: decide Y  âŒ (violaÃ§Ã£o - decidiu 2 vezes)
```

## O Problema: FLP Impossibility

**Teorema FLP** (Fischer, Lynch, Paterson, 1985):

> Em um sistema distribuÃ­do assÃ­ncrono, Ã© **impossÃ­vel** garantir consenso se **pelo menos um processo pode falhar**, mesmo que apenas por crash.

### O que isso significa?

```
NÃ£o podemos ter simultaneamente:
1. Safety (acordo correto)
2. Liveness (tÃ©rmino garantido)
3. TolerÃ¢ncia a falhas

Em rede assÃ­ncrona, temos que escolher!
```

**SoluÃ§Ã£o prÃ¡tica**: Usar suposiÃ§Ãµes adicionais (timeouts, maioria dos nÃ³s funcionando, etc).

## Algoritmos de Consenso

### 1. Two-Phase Commit (2PC)

**Uso**: TransaÃ§Ãµes distribuÃ­das (ACID).

**Fases**:

#### Fase 1: Prepare (VotaÃ§Ã£o)

```
Coordinator               Participant A        Participant B
    â”‚                          â”‚                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€ PREPARE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€ PREPARE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
    â”‚                          â”‚                     â”‚
    â”‚â†â”€â”€â”€â”€â”€â”€ VOTE_YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
    â”‚â†â”€â”€â”€â”€â”€â”€ VOTE_YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

#### Fase 2: Commit/Abort

```
Coordinator               Participant A        Participant B
    â”‚                          â”‚                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€ COMMIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€ COMMIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
    â”‚                          â”‚                     â”‚
    â”‚â†â”€â”€â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
    â”‚â†â”€â”€â”€â”€â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

**ImplementaÃ§Ã£o**:

```python
class TwoPhaseCommit:
    def __init__(self, coordinator_id, participants):
        self.coordinator_id = coordinator_id
        self.participants = participants

    def execute_transaction(self, transaction):
        # Fase 1: PREPARE
        print("Phase 1: Prepare")
        votes = []

        for participant in self.participants:
            vote = participant.prepare(transaction)
            votes.append(vote)

            if vote == 'ABORT':
                # Se qualquer participante vota ABORT, abortar tudo
                self.abort_transaction()
                return False

        # Todos votaram YES
        print("Phase 2: Commit")

        # Fase 2: COMMIT
        for participant in self.participants:
            participant.commit(transaction)

        return True

    def abort_transaction(self):
        print("Aborting transaction")
        for participant in self.participants:
            participant.abort()

# Participante
class Participant:
    def __init__(self, participant_id):
        self.participant_id = participant_id
        self.prepared_transaction = None

    def prepare(self, transaction):
        # Verificar se pode executar transaÃ§Ã£o
        if self.can_execute(transaction):
            # Salvar em log (para recovery)
            self.prepared_transaction = transaction
            return 'YES'
        else:
            return 'ABORT'

    def commit(self, transaction):
        # Executar transaÃ§Ã£o
        self.execute(transaction)
        self.prepared_transaction = None

    def abort(self):
        # Desfazer preparaÃ§Ã£o
        self.prepared_transaction = None
```

**Problema: Single Point of Failure**

```
Coordinator envia PREPARE
Todos respondem YES
Coordinator FALHA antes de enviar COMMIT â† Participantes bloqueados!

Participantes ficam esperando forever (ou atÃ© timeout)
```

**Vantagens**:

- âœ… Garante atomicidade
- âœ… Simples de entender

**Desvantagens**:

- âŒ Blocking protocol (participantes podem ficar bloqueados)
- âŒ Coordinator Ã© single point of failure
- âŒ NÃ£o tolera partiÃ§Ãµes de rede

### 2. Three-Phase Commit (3PC)

**Melhoria do 2PC**: Adiciona fase intermediÃ¡ria para evitar blocking.

**Fases**:

1. **CanCommit**: Perguntar se podem commitar
2. **PreCommit**: Preparar para commit
3. **DoCommit**: Executar commit

**Vantagem**: Permite participantes fazerem progresso mesmo se coordinator falhar.

**Desvantagem**: Mais complexo, mais latÃªncia (3 fases em vez de 2).

### 3. Paxos

**Algoritmo de consenso tolerante a falhas.**

**Roles**:

- **Proposer**: PropÃµe valores
- **Acceptor**: Aceita propostas
- **Learner**: Aprende valor decidido

**Fases**:

#### Fase 1: Prepare

```
Proposer                  Acceptor 1    Acceptor 2    Acceptor 3
   â”‚                          â”‚             â”‚             â”‚
   â”œâ”€ PREPARE(n=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚             â”‚             â”‚
   â”œâ”€ PREPARE(n=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚             â”‚
   â”œâ”€ PREPARE(n=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
   â”‚                          â”‚             â”‚             â”‚
   â”‚â† PROMISE(n=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚
   â”‚â† PROMISE(n=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
   â”‚â† PROMISE(n=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

#### Fase 2: Accept

```
Proposer                  Acceptor 1    Acceptor 2    Acceptor 3
   â”‚                          â”‚             â”‚             â”‚
   â”œâ”€ ACCEPT(n=1, v=X) â”€â”€â”€â”€â”€â”€â†’â”‚             â”‚             â”‚
   â”œâ”€ ACCEPT(n=1, v=X) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚             â”‚
   â”œâ”€ ACCEPT(n=1, v=X) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
   â”‚                          â”‚             â”‚             â”‚
   â”‚â† ACCEPTED(n=1, v=X) â”€â”€â”€â”€â”€â”¤             â”‚             â”‚
   â”‚â† ACCEPTED(n=1, v=X) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
   â”‚â† ACCEPTED(n=1, v=X) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

**CaracterÃ­sticas**:

- âœ… Tolerante a falhas (funciona com maioria)
- âœ… NÃ£o tem blocking (eventual progress)
- âŒ Complexo de entender
- âŒ DifÃ­cil de implementar corretamente
- âŒ Pode ter livelock (proposers competindo)

**Usado em**:

- Google Chubby
- Apache Zookeeper (variante: Zab)

### 4. Raft

**Consenso mais fÃ¡cil de entender** (projetado para ser mais simples que Paxos).

**Componentes**:

- **Leader**: Recebe requisiÃ§Ãµes e replica para followers
- **Follower**: Replica estado do leader
- **Candidate**: Candidato a lÃ­der durante eleiÃ§Ã£o

**Estados**:

```
       start/timeout
           â†“
      [Follower] â”€â”€â”€â”€timeoutâ”€â”€â”€â”€â†’ [Candidate]
           â†‘                            â”‚
           â”‚                            â”‚
           â”‚                     votes from majority
           â”‚                            â†“
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  [Leader]
                 discovers leader
                 or new term
```

#### Leader Election

**Processo**:

```
1. Follower nÃ£o recebe heartbeat â†’ Vira Candidate
2. Candidate incrementa term, vota em si mesmo
3. Candidate pede votos a outros nÃ³s
4. Se recebe maioria â†’ Vira Leader
5. Leader envia heartbeats periÃ³dicos
```

**CÃ³digo simplificado**:

```python
class RaftNode:
    def __init__(self, node_id, peers):
        self.node_id = node_id
        self.peers = peers
        self.state = 'FOLLOWER'
        self.current_term = 0
        self.voted_for = None
        self.log = []
        self.commit_index = 0

    def start_election(self):
        """Candidate inicia eleiÃ§Ã£o"""
        self.state = 'CANDIDATE'
        self.current_term += 1
        self.voted_for = self.node_id

        votes = 1  # Voto em si mesmo

        # Pedir votos
        for peer in self.peers:
            if peer.request_vote(self.current_term, self.node_id):
                votes += 1

        # Maioria?
        if votes > len(self.peers) / 2:
            self.become_leader()

    def become_leader(self):
        """Tornar-se lÃ­der"""
        self.state = 'LEADER'
        print(f"Node {self.node_id} became leader for term {self.current_term}")

        # Enviar heartbeats
        self.send_heartbeats()

    def request_vote(self, term, candidate_id):
        """Receber pedido de voto"""
        if term > self.current_term:
            # Termo mais recente, resetar
            self.current_term = term
            self.voted_for = None
            self.state = 'FOLLOWER'

        if self.voted_for is None or self.voted_for == candidate_id:
            self.voted_for = candidate_id
            return True

        return False

    def append_entries(self, term, leader_id, entries):
        """Receber append entries (heartbeat ou log replication)"""
        if term < self.current_term:
            return False

        # Reconhecer lÃ­der
        self.current_term = term
        self.state = 'FOLLOWER'

        # Adicionar entradas ao log
        self.log.extend(entries)

        return True
```

#### Log Replication

**Processo**:

```
Cliente â†’ Leader: SET x = 5

Leader:
1. Adiciona entry ao seu log [term=1, cmd="SET x=5"]
2. Envia AppendEntries para Followers

Followers:
1. Recebem entry
2. Adicionam ao log
3. Respondem OK

Leader:
1. Recebe maioria de OKs
2. Commita entry (aplica ao state machine)
3. Responde ao cliente
4. Notifica Followers para commitarem
```

**CÃ³digo**:

```python
class RaftLeader(RaftNode):
    def replicate_log(self, command):
        """Replicar comando para followers"""
        # Adicionar ao log local
        entry = LogEntry(
            term=self.current_term,
            command=command
        )
        self.log.append(entry)

        # Replicar para followers
        acks = 1  # Leader jÃ¡ tem

        for follower in self.peers:
            success = follower.append_entries(
                term=self.current_term,
                leader_id=self.node_id,
                entries=[entry]
            )

            if success:
                acks += 1

        # Maioria confirmou?
        if acks > len(self.peers) / 2:
            # Commitar
            self.commit_index += 1
            self.apply_to_state_machine(entry)
            return True

        return False
```

**CaracterÃ­sticas do Raft**:

- âœ… Mais fÃ¡cil de entender que Paxos
- âœ… SeparaÃ§Ã£o clara entre leader election e log replication
- âœ… Strong leader (simplifica replicaÃ§Ã£o)
- âœ… Bem documentado

**Usado em**:

- etcd
- Consul
- CockroachDB (variante)

### ComparaÃ§Ã£o: Paxos vs Raft

| Aspecto        | Paxos            | Raft                             |
| -------------- | ---------------- | -------------------------------- |
| Complexidade   | Alta             | MÃ©dia                            |
| Leader         | NÃ£o necessÃ¡rio   | Sempre hÃ¡ lÃ­der                  |
| Fases          | 2 fases          | 2 fases (election + replication) |
| CompreensÃ£o    | DifÃ­cil          | Mais fÃ¡cil                       |
| ImplementaÃ§Ãµes | Muitas variantes | Mais consistente                 |

## AplicaÃ§Ãµes de Consenso

### 1. Distributed Databases

```
CockroachDB, TiDB
â†’ Usar Raft para replicaÃ§Ã£o
â†’ Garantir consistÃªncia entre replicas
```

### 2. Configuration Management

```
etcd, Consul, ZooKeeper
â†’ Armazenar configuraÃ§Ãµes
â†’ Garantir que todos veem mesma config
```

### 3. Distributed Locks

```
Usar consenso para decidir quem tem o lock
â†’ Apenas um nÃ³ pode ter lock por vez
```

### 4. Service Discovery

```
Registrar serviÃ§os no cluster
â†’ Todos concordam sobre quais serviÃ§os estÃ£o ativos
```

## Quorum

**Quorum**: NÃºmero mÃ­nimo de nÃ³s necessÃ¡rios para operaÃ§Ã£o.

**FÃ³rmula**:

```
Quorum = floor(N / 2) + 1

N = 3 â†’ Quorum = 2
N = 5 â†’ Quorum = 3
N = 7 â†’ Quorum = 4
```

**Por que maioria?**

```
Cluster com 5 nÃ³s: [A, B, C, D, E]
Quorum = 3

PartiÃ§Ã£o:
Partition 1: [A, B, C] â†’ 3 nÃ³s âœ… Tem quorum
Partition 2: [D, E]    â†’ 2 nÃ³s âŒ NÃ£o tem quorum

Apenas Partition 1 pode fazer progresso
â†’ ImpossÃ­vel ter 2 partiÃ§Ãµes com quorum simultaneamente
â†’ Evita split brain!
```

## Write e Read Quorums

**ConfigurÃ¡vel em alguns sistemas** (Cassandra, DynamoDB):

```
N = nÃºmero total de replicas
W = write quorum (quantos devem confirmar escrita)
R = read quorum (quantos devem responder leitura)

Se W + R > N â†’ Leitura sempre vÃª escrita mais recente
```

**Exemplos**:

```
N = 3, W = 2, R = 2
â†’ W + R = 4 > 3 âœ… Strong consistency

N = 3, W = 1, R = 1
â†’ W + R = 2 < 3 âŒ Eventual consistency

N = 3, W = 3, R = 1
â†’ Alta consistÃªncia de escrita, leitura rÃ¡pida

N = 3, W = 1, R = 3
â†’ Escrita rÃ¡pida, alta consistÃªncia de leitura
```

## Desafios do Consenso

### 1. Performance

**Consenso adiciona latÃªncia**:

```
Sem consenso: 1 RTT (cliente â†’ servidor)
Com consenso: 2-3 RTTs (prepare + accept + commit)
```

**MitigaÃ§Ã£o**:

- Pipelining de requests
- Batching mÃºltiplas operaÃ§Ãµes
- Read from followers (relaxar consistÃªncia)

### 2. NÃºmero de NÃ³s

**Mais nÃ³s = mais latÃªncia**:

```
3 nÃ³s: Precisa 2 ACKs (rÃ¡pido)
7 nÃ³s: Precisa 4 ACKs (mais lento)
```

**Trade-off**:

```
Mais nÃ³s â†’ Mais tolerÃ¢ncia a falhas
Mais nÃ³s â†’ Mais latÃªncia

Sweet spot: 3, 5, ou 7 nÃ³s
```

### 3. Network Partitions

**Durante partiÃ§Ã£o**:

```
Apenas partiÃ§Ã£o com maioria faz progresso
PartiÃ§Ã£o minoritÃ¡ria fica bloqueada

Se nenhuma partiÃ§Ã£o tem maioria â†’ Sistema para (safety over liveness)
```

## ImplementaÃ§Ãµes PrÃ¡ticas

### etcd (usando Raft)

```go
package main

import (
    "context"
    "go.etcd.io/etcd/client/v3"
)

func main() {
    // Conectar ao cluster etcd
    cli, _ := clientv3.New(clientv3.Config{
        Endpoints: []string{"localhost:2379"},
    })
    defer cli.Close()

    // Write (replicado via Raft)
    cli.Put(context.Background(), "key", "value")

    // Read
    resp, _ := cli.Get(context.Background(), "key")

    // Consenso garante que todos veem mesmo valor
}
```

### Consul (usando Raft)

```python
import consul

c = consul.Consul()

# Write (replicado via Raft)
c.kv.put('config/db/host', 'localhost')

# Read
index, data = c.kv.get('config/db/host')

# Consenso garante consistÃªncia
```

## Best Practices

### âœ… Do:

1. **Use implementaÃ§Ãµes existentes** (etcd, Consul, ZooKeeper)
2. **Use nÃºmero Ã­mpar de nÃ³s** (3, 5, 7)
3. **Considere latÃªncia** entre nÃ³s (coloque no mesmo datacenter)
4. **Monitore health** do cluster
5. **Teste partiÃ§Ãµes de rede**

### âŒ Don't:

1. **NÃ£o implemente consenso do zero** (muito difÃ­cil acertar)
2. **NÃ£o use nÃºmero par de nÃ³s** (nÃ£o ajuda tolerÃ¢ncia a falhas)
3. **NÃ£o espalhe nÃ³s geograficamente demais** (latÃªncia alta)
4. **NÃ£o ignore degradaÃ§Ã£o de performance**

## Exemplo PrÃ¡tico: Distributed Counter

```python
import etcd3

class DistributedCounter:
    def __init__(self, key='counter'):
        self.etcd = etcd3.client()
        self.key = key

    def increment(self):
        """Incrementar contador (linearizable)"""
        while True:
            # Read current value
            value, metadata = self.etcd.get(self.key)
            current = int(value) if value else 0

            # Tentar update com compare-and-swap
            success = self.etcd.transaction(
                compare=[
                    self.etcd.transactions.value(self.key) == value
                ],
                success=[
                    self.etcd.transactions.put(self.key, str(current + 1))
                ],
                failure=[]
            )

            if success:
                return current + 1

            # CAS falhou, retry

    def get(self):
        """Ler contador"""
        value, _ = self.etcd.get(self.key)
        return int(value) if value else 0

# Uso: MÃºltiplos processos podem incrementar concorrentemente
# etcd (usando Raft) garante que increments nÃ£o sÃ£o perdidos

counter = DistributedCounter()
new_value = counter.increment()  # Linearizable!
```

## ConclusÃ£o

**Consenso** Ã© fundamental para sistemas distribuÃ­dos que exigem forte consistÃªncia:

- **Use quando**: NecessÃ¡rio concordÃ¢ncia sobre estado (databases, config)
- **Evite quando**: Performance Ã© mais importante que consistÃªncia

**Key Takeaway**: NÃ£o implemente consenso do zero. Use sistemas maduros (etcd, Consul, ZooKeeper) que jÃ¡ resolveram esses problemas complexos.

---

ðŸŽ‰ **ParabÃ©ns!** VocÃª completou a seÃ§Ã£o de **Sistemas DistribuÃ­dos**!

Continue explorando outros tÃ³picos de fundamentos e arquitetura.
