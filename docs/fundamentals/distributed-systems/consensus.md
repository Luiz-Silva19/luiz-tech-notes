---
id: distributed-systems-consensus
title: Consenso
sidebar_label: Consenso
---

## O que √© Consenso em Sistemas Distribu√≠dos?

**Consenso** √© o processo de fazer m√∫ltiplos n√≥s em um sistema distribu√≠do concordarem sobre um √∫nico valor ou estado, mesmo na presen√ßa de falhas.

## Por que Consenso √© Importante?

### Problemas que Consenso Resolve

#### 1. Replica√ß√£o de Estado

```
Node A: balance = $100
Node B: balance = $100  ‚Üê Todos concordam sobre saldo
Node C: balance = $100
```

#### 2. Ordem de Opera√ß√µes

```
Cliente 1: SET x = 1
Cliente 2: SET x = 2

Todos os n√≥s devem concordar: qual opera√ß√£o vem primeiro?
```

#### 3. Membership Changes

```
Cluster: [A, B, C]

Adicionar D
‚Üí Todos devem concordar que cluster agora √© [A, B, C, D]
```

#### 4. Leader Election

```
M√∫ltiplos n√≥s candidatos a l√≠der
‚Üí Consenso sobre quem √© o l√≠der
```

## Propriedades do Consenso

### 1. Agreement (Acordo)

Todos os n√≥s corretos decidem o **mesmo valor**.

```
Node A decide: X
Node B decide: X  ‚Üê Mesmo valor
Node C decide: X
```

### 2. Validity (Validade)

Se todos os n√≥s prop√µem o mesmo valor V, ent√£o a decis√£o √© V.

```
Todos prop√µem: X
Decis√£o: X ‚úÖ

Todos prop√µem: X
Decis√£o: Y ‚ùå (viola√ß√£o)
```

### 3. Termination (T√©rmino)

Todo n√≥ correto eventualmente decide algum valor.

```
Algoritmo n√£o pode ficar bloqueado forever
```

### 4. Integrity (Integridade)

Cada n√≥ decide no m√°ximo uma vez.

```
Node A: decide X
Node A: decide Y  ‚ùå (viola√ß√£o - decidiu 2 vezes)
```

## O Problema: FLP Impossibility

**Teorema FLP** (Fischer, Lynch, Paterson, 1985):

> Em um sistema distribu√≠do ass√≠ncrono, √© **imposs√≠vel** garantir consenso se **pelo menos um processo pode falhar**, mesmo que apenas por crash.

### O que isso significa?

```
N√£o podemos ter simultaneamente:
1. Safety (acordo correto)
2. Liveness (t√©rmino garantido)
3. Toler√¢ncia a falhas

Em rede ass√≠ncrona, temos que escolher!
```

**Solu√ß√£o pr√°tica**: Usar suposi√ß√µes adicionais (timeouts, maioria dos n√≥s funcionando, etc).

## Algoritmos de Consenso

### 1. Two-Phase Commit (2PC)

**Uso**: Transa√ß√µes distribu√≠das (ACID).

**Fases**:

#### Fase 1: Prepare (Vota√ß√£o)

```
Coordinator               Participant A        Participant B
    ‚îÇ                          ‚îÇ                     ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PREPARE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ                     ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PREPARE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ
    ‚îÇ                          ‚îÇ                     ‚îÇ
    ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ VOTE_YES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                     ‚îÇ
    ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ VOTE_YES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
```

#### Fase 2: Commit/Abort

```
Coordinator               Participant A        Participant B
    ‚îÇ                          ‚îÇ                     ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMMIT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ                     ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ COMMIT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ
    ‚îÇ                          ‚îÇ                     ‚îÇ
    ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ACK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                     ‚îÇ
    ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ACK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
```

**Implementa√ß√£o**:

```python
class TwoPhaseCommit:
    def __init__(self, coordinator_id, participants):
        self.coordinator_id = coordinator_id
        self.participants = participants

    def execute_transaction(self, transaction):
        # Fase 1: PREPARE
        print("Phase 1: Prepare")
        votes = []

        for participant in self.participants:
            vote = participant.prepare(transaction)
            votes.append(vote)

            if vote == 'ABORT':
                # Se qualquer participante vota ABORT, abortar tudo
                self.abort_transaction()
                return False

        # Todos votaram YES
        print("Phase 2: Commit")

        # Fase 2: COMMIT
        for participant in self.participants:
            participant.commit(transaction)

        return True

    def abort_transaction(self):
        print("Aborting transaction")
        for participant in self.participants:
            participant.abort()

# Participante
class Participant:
    def __init__(self, participant_id):
        self.participant_id = participant_id
        self.prepared_transaction = None

    def prepare(self, transaction):
        # Verificar se pode executar transa√ß√£o
        if self.can_execute(transaction):
            # Salvar em log (para recovery)
            self.prepared_transaction = transaction
            return 'YES'
        else:
            return 'ABORT'

    def commit(self, transaction):
        # Executar transa√ß√£o
        self.execute(transaction)
        self.prepared_transaction = None

    def abort(self):
        # Desfazer prepara√ß√£o
        self.prepared_transaction = None
```

**Problema: Single Point of Failure**

```
Coordinator envia PREPARE
Todos respondem YES
Coordinator FALHA antes de enviar COMMIT ‚Üê Participantes bloqueados!

Participantes ficam esperando forever (ou at√© timeout)
```

**Vantagens**:

- ‚úÖ Garante atomicidade
- ‚úÖ Simples de entender

**Desvantagens**:

- ‚ùå Blocking protocol (participantes podem ficar bloqueados)
- ‚ùå Coordinator √© single point of failure
- ‚ùå N√£o tolera parti√ß√µes de rede

### 2. Three-Phase Commit (3PC)

**Melhoria do 2PC**: Adiciona fase intermedi√°ria para evitar blocking.

**Fases**:

1. **CanCommit**: Perguntar se podem commitar
2. **PreCommit**: Preparar para commit
3. **DoCommit**: Executar commit

**Vantagem**: Permite participantes fazerem progresso mesmo se coordinator falhar.

**Desvantagem**: Mais complexo, mais lat√™ncia (3 fases em vez de 2).

### 3. Paxos

**Algoritmo de consenso tolerante a falhas.**

**Roles**:

- **Proposer**: Prop√µe valores
- **Acceptor**: Aceita propostas
- **Learner**: Aprende valor decidido

**Fases**:

#### Fase 1: Prepare

```
Proposer                  Acceptor 1    Acceptor 2    Acceptor 3
   ‚îÇ                          ‚îÇ             ‚îÇ             ‚îÇ
   ‚îú‚îÄ PREPARE(n=1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ             ‚îÇ             ‚îÇ
   ‚îú‚îÄ PREPARE(n=1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ             ‚îÇ
   ‚îú‚îÄ PREPARE(n=1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ
   ‚îÇ                          ‚îÇ             ‚îÇ             ‚îÇ
   ‚îÇ‚Üê PROMISE(n=1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ             ‚îÇ
   ‚îÇ‚Üê PROMISE(n=1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ
   ‚îÇ‚Üê PROMISE(n=1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
```

#### Fase 2: Accept

```
Proposer                  Acceptor 1    Acceptor 2    Acceptor 3
   ‚îÇ                          ‚îÇ             ‚îÇ             ‚îÇ
   ‚îú‚îÄ ACCEPT(n=1, v=X) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ             ‚îÇ             ‚îÇ
   ‚îú‚îÄ ACCEPT(n=1, v=X) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ             ‚îÇ
   ‚îú‚îÄ ACCEPT(n=1, v=X) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ
   ‚îÇ                          ‚îÇ             ‚îÇ             ‚îÇ
   ‚îÇ‚Üê ACCEPTED(n=1, v=X) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ             ‚îÇ
   ‚îÇ‚Üê ACCEPTED(n=1, v=X) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ
   ‚îÇ‚Üê ACCEPTED(n=1, v=X) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
```

**Caracter√≠sticas**:

- ‚úÖ Tolerante a falhas (funciona com maioria)
- ‚úÖ N√£o tem blocking (eventual progress)
- ‚ùå Complexo de entender
- ‚ùå Dif√≠cil de implementar corretamente
- ‚ùå Pode ter livelock (proposers competindo)

**Usado em**:

- Google Chubby
- Apache Zookeeper (variante: Zab)

### 4. Raft

**Consenso mais f√°cil de entender** (projetado para ser mais simples que Paxos).

**Componentes**:

- **Leader**: Recebe requisi√ß√µes e replica para followers
- **Follower**: Replica estado do leader
- **Candidate**: Candidato a l√≠der durante elei√ß√£o

**Estados**:

```
       start/timeout
           ‚Üì
      [Follower] ‚îÄ‚îÄ‚îÄ‚îÄtimeout‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [Candidate]
           ‚Üë                            ‚îÇ
           ‚îÇ                            ‚îÇ
           ‚îÇ                     votes from majority
           ‚îÇ                            ‚Üì
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  [Leader]
                 discovers leader
                 or new term
```

#### Leader Election

**Processo**:

```
1. Follower n√£o recebe heartbeat ‚Üí Vira Candidate
2. Candidate incrementa term, vota em si mesmo
3. Candidate pede votos a outros n√≥s
4. Se recebe maioria ‚Üí Vira Leader
5. Leader envia heartbeats peri√≥dicos
```

**C√≥digo simplificado**:

```python
class RaftNode:
    def __init__(self, node_id, peers):
        self.node_id = node_id
        self.peers = peers
        self.state = 'FOLLOWER'
        self.current_term = 0
        self.voted_for = None
        self.log = []
        self.commit_index = 0

    def start_election(self):
        """Candidate inicia elei√ß√£o"""
        self.state = 'CANDIDATE'
        self.current_term += 1
        self.voted_for = self.node_id

        votes = 1  # Voto em si mesmo

        # Pedir votos
        for peer in self.peers:
            if peer.request_vote(self.current_term, self.node_id):
                votes += 1

        # Maioria?
        if votes > len(self.peers) / 2:
            self.become_leader()

    def become_leader(self):
        """Tornar-se l√≠der"""
        self.state = 'LEADER'
        print(f"Node {self.node_id} became leader for term {self.current_term}")

        # Enviar heartbeats
        self.send_heartbeats()

    def request_vote(self, term, candidate_id):
        """Receber pedido de voto"""
        if term > self.current_term:
            # Termo mais recente, resetar
            self.current_term = term
            self.voted_for = None
            self.state = 'FOLLOWER'

        if self.voted_for is None or self.voted_for == candidate_id:
            self.voted_for = candidate_id
            return True

        return False

    def append_entries(self, term, leader_id, entries):
        """Receber append entries (heartbeat ou log replication)"""
        if term < self.current_term:
            return False

        # Reconhecer l√≠der
        self.current_term = term
        self.state = 'FOLLOWER'

        # Adicionar entradas ao log
        self.log.extend(entries)

        return True
```

#### Log Replication

**Processo**:

```
Cliente ‚Üí Leader: SET x = 5

Leader:
1. Adiciona entry ao seu log [term=1, cmd="SET x=5"]
2. Envia AppendEntries para Followers

Followers:
1. Recebem entry
2. Adicionam ao log
3. Respondem OK

Leader:
1. Recebe maioria de OKs
2. Commita entry (aplica ao state machine)
3. Responde ao cliente
4. Notifica Followers para commitarem
```

**C√≥digo**:

```python
class RaftLeader(RaftNode):
    def replicate_log(self, command):
        """Replicar comando para followers"""
        # Adicionar ao log local
        entry = LogEntry(
            term=self.current_term,
            command=command
        )
        self.log.append(entry)

        # Replicar para followers
        acks = 1  # Leader j√° tem

        for follower in self.peers:
            success = follower.append_entries(
                term=self.current_term,
                leader_id=self.node_id,
                entries=[entry]
            )

            if success:
                acks += 1

        # Maioria confirmou?
        if acks > len(self.peers) / 2:
            # Commitar
            self.commit_index += 1
            self.apply_to_state_machine(entry)
            return True

        return False
```

**Caracter√≠sticas do Raft**:

- ‚úÖ Mais f√°cil de entender que Paxos
- ‚úÖ Separa√ß√£o clara entre leader election e log replication
- ‚úÖ Strong leader (simplifica replica√ß√£o)
- ‚úÖ Bem documentado

**Usado em**:

- etcd
- Consul
- CockroachDB (variante)

### Compara√ß√£o: Paxos vs Raft

| Aspecto        | Paxos            | Raft                             |
| -------------- | ---------------- | -------------------------------- |
| Complexidade   | Alta             | M√©dia                            |
| Leader         | N√£o necess√°rio   | Sempre h√° l√≠der                  |
| Fases          | 2 fases          | 2 fases (election + replication) |
| Compreens√£o    | Dif√≠cil          | Mais f√°cil                       |
| Implementa√ß√µes | Muitas variantes | Mais consistente                 |

## Aplica√ß√µes de Consenso

### 1. Distributed Databases

```
CockroachDB, TiDB
‚Üí Usar Raft para replica√ß√£o
‚Üí Garantir consist√™ncia entre replicas
```

### 2. Configuration Management

```
etcd, Consul, ZooKeeper
‚Üí Armazenar configura√ß√µes
‚Üí Garantir que todos veem mesma config
```

### 3. Distributed Locks

```
Usar consenso para decidir quem tem o lock
‚Üí Apenas um n√≥ pode ter lock por vez
```

### 4. Service Discovery

```
Registrar servi√ßos no cluster
‚Üí Todos concordam sobre quais servi√ßos est√£o ativos
```

## Quorum

**Quorum**: N√∫mero m√≠nimo de n√≥s necess√°rios para opera√ß√£o.

**F√≥rmula**:

```
Quorum = floor(N / 2) + 1

N = 3 ‚Üí Quorum = 2
N = 5 ‚Üí Quorum = 3
N = 7 ‚Üí Quorum = 4
```

**Por que maioria?**

```
Cluster com 5 n√≥s: [A, B, C, D, E]
Quorum = 3

Parti√ß√£o:
Partition 1: [A, B, C] ‚Üí 3 n√≥s ‚úÖ Tem quorum
Partition 2: [D, E]    ‚Üí 2 n√≥s ‚ùå N√£o tem quorum

Apenas Partition 1 pode fazer progresso
‚Üí Imposs√≠vel ter 2 parti√ß√µes com quorum simultaneamente
‚Üí Evita split brain!
```

## Write e Read Quorums

**Configur√°vel em alguns sistemas** (Cassandra, DynamoDB):

```
N = n√∫mero total de replicas
W = write quorum (quantos devem confirmar escrita)
R = read quorum (quantos devem responder leitura)

Se W + R > N ‚Üí Leitura sempre v√™ escrita mais recente
```

**Exemplos**:

```
N = 3, W = 2, R = 2
‚Üí W + R = 4 > 3 ‚úÖ Strong consistency

N = 3, W = 1, R = 1
‚Üí W + R = 2 < 3 ‚ùå Eventual consistency

N = 3, W = 3, R = 1
‚Üí Alta consist√™ncia de escrita, leitura r√°pida

N = 3, W = 1, R = 3
‚Üí Escrita r√°pida, alta consist√™ncia de leitura
```

## Desafios do Consenso

### 1. Performance

**Consenso adiciona lat√™ncia**:

```
Sem consenso: 1 RTT (cliente ‚Üí servidor)
Com consenso: 2-3 RTTs (prepare + accept + commit)
```

**Mitiga√ß√£o**:

- Pipelining de requests
- Batching m√∫ltiplas opera√ß√µes
- Read from followers (relaxar consist√™ncia)

### 2. N√∫mero de N√≥s

**Mais n√≥s = mais lat√™ncia**:

```
3 n√≥s: Precisa 2 ACKs (r√°pido)
7 n√≥s: Precisa 4 ACKs (mais lento)
```

**Trade-off**:

```
Mais n√≥s ‚Üí Mais toler√¢ncia a falhas
Mais n√≥s ‚Üí Mais lat√™ncia

Sweet spot: 3, 5, ou 7 n√≥s
```

### 3. Network Partitions

**Durante parti√ß√£o**:

```
Apenas parti√ß√£o com maioria faz progresso
Parti√ß√£o minorit√°ria fica bloqueada

Se nenhuma parti√ß√£o tem maioria ‚Üí Sistema para (safety over liveness)
```

## Implementa√ß√µes Pr√°ticas

### etcd (usando Raft)

```go
package main

import (
    "context"
    "go.etcd.io/etcd/client/v3"
)

func main() {
    // Conectar ao cluster etcd
    cli, _ := clientv3.New(clientv3.Config{
        Endpoints: []string{"localhost:2379"},
    })
    defer cli.Close()

    // Write (replicado via Raft)
    cli.Put(context.Background(), "key", "value")

    // Read
    resp, _ := cli.Get(context.Background(), "key")

    // Consenso garante que todos veem mesmo valor
}
```

### Consul (usando Raft)

```python
import consul

c = consul.Consul()

# Write (replicado via Raft)
c.kv.put('config/db/host', 'localhost')

# Read
index, data = c.kv.get('config/db/host')

# Consenso garante consist√™ncia
```

## Best Practices

### ‚úÖ Do:

1. **Use implementa√ß√µes existentes** (etcd, Consul, ZooKeeper)
2. **Use n√∫mero √≠mpar de n√≥s** (3, 5, 7)
3. **Considere lat√™ncia** entre n√≥s (coloque no mesmo datacenter)
4. **Monitore health** do cluster
5. **Teste parti√ß√µes de rede**

### ‚ùå Don't:

1. **N√£o implemente consenso do zero** (muito dif√≠cil acertar)
2. **N√£o use n√∫mero par de n√≥s** (n√£o ajuda toler√¢ncia a falhas)
3. **N√£o espalhe n√≥s geograficamente demais** (lat√™ncia alta)
4. **N√£o ignore degrada√ß√£o de performance**

## Exemplo Pr√°tico: Distributed Counter

```python
import etcd3

class DistributedCounter:
    def __init__(self, key='counter'):
        self.etcd = etcd3.client()
        self.key = key

    def increment(self):
        """Incrementar contador (linearizable)"""
        while True:
            # Read current value
            value, metadata = self.etcd.get(self.key)
            current = int(value) if value else 0

            # Tentar update com compare-and-swap
            success = self.etcd.transaction(
                compare=[
                    self.etcd.transactions.value(self.key) == value
                ],
                success=[
                    self.etcd.transactions.put(self.key, str(current + 1))
                ],
                failure=[]
            )

            if success:
                return current + 1

            # CAS falhou, retry

    def get(self):
        """Ler contador"""
        value, _ = self.etcd.get(self.key)
        return int(value) if value else 0

# Uso: M√∫ltiplos processos podem incrementar concorrentemente
# etcd (usando Raft) garante que increments n√£o s√£o perdidos

counter = DistributedCounter()
new_value = counter.increment()  # Linearizable!
```

## Conclus√£o

**Consenso** √© fundamental para sistemas distribu√≠dos que exigem forte consist√™ncia:

- **Use quando**: Necess√°rio concord√¢ncia sobre estado (databases, config)
- **Evite quando**: Performance √© mais importante que consist√™ncia

**Key Takeaway**: N√£o implemente consenso do zero. Use sistemas maduros (etcd, Consul, ZooKeeper) que j√° resolveram esses problemas complexos.

## üìö Refer√™ncias e Recursos

### Papers Fundamentais

- **<a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf" target="_blank" rel="noopener noreferrer">Paxos Made Simple</a>** - Leslie Lamport (2001)
- **<a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener noreferrer">In Search of an Understandable Consensus Algorithm (Raft)</a>** - Ongaro & Ousterhout (2014)
- **<a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf" target="_blank" rel="noopener noreferrer">Impossibility of Distributed Consensus with One Faulty Process (FLP)</a>** - Fischer, Lynch, Paterson (1985)
- **<a href="https://pmg.csail.mit.edu/papers/vr.pdf" target="_blank" rel="noopener noreferrer">Viewstamped Replication</a>** - Oki & Liskov (1988)

### Implementa√ß√µes

- **<a href="https://etcd.io/docs/" target="_blank" rel="noopener noreferrer">etcd Documentation</a>** - Raft-based distributed key-value store
- **<a href="https://www.consul.io/docs" target="_blank" rel="noopener noreferrer">Consul by HashiCorp</a>** - Service mesh com consenso
- **<a href="https://zookeeper.apache.org/doc/current/" target="_blank" rel="noopener noreferrer">Apache ZooKeeper</a>** - Coordination service (Zab protocol)
- **<a href="https://www.cockroachlabs.com/docs/" target="_blank" rel="noopener noreferrer">CockroachDB</a>** - Distributed SQL com Raft

### Visualiza√ß√µes e Tutoriais

- **<a href="https://raft.github.io/" target="_blank" rel="noopener noreferrer">Raft Visualization</a>** - Visualiza√ß√£o interativa do Raft
- **<a href="http://thesecretlivesofdata.com/raft/" target="_blank" rel="noopener noreferrer">The Secret Lives of Data</a>** - Anima√ß√£o do Raft
- **<a href="https://research.google/pubs/pub33002/" target="_blank" rel="noopener noreferrer">Paxos Made Live</a>** - Google Chubby experience

### Artigos T√©cnicos

- **<a href="https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/" target="_blank" rel="noopener noreferrer">Consensus Protocols: Two-Phase Commit</a>** - The Paper Trail
- **<a href="https://www.the-paper-trail.org/post/2008-11-29-consensus-protocols-three-phase-commit/" target="_blank" rel="noopener noreferrer">Consensus Protocols: Three-phase Commit</a>** - The Paper Trail
- **<a href="https://www.the-paper-trail.org/post/2009-02-03-consensus-protocols-paxos/" target="_blank" rel="noopener noreferrer">Consensus Protocols: Paxos</a>** - The Paper Trail
- **<a href="https://understandingpaxos.wordpress.com/" target="_blank" rel="noopener noreferrer">Understanding Paxos</a>** - Tutorial completo

### Cursos e Palestras

- **<a href="https://pdos.csail.mit.edu/6.824/" target="_blank" rel="noopener noreferrer">MIT 6.824 Distributed Systems</a>** - Labs implementando Raft
- **<a href="https://www.youtube.com/watch?v=YbZ3zDzDnrw" target="_blank" rel="noopener noreferrer">Raft Lecture by Diego Ongaro</a>** - Criador do Raft

---

üéâ **Parab√©ns!** Voc√™ completou a se√ß√£o de **Sistemas Distribu√≠dos**!

Continue explorando outros t√≥picos de fundamentos e arquitetura.
